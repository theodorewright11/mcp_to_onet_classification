{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a001md00",
   "metadata": {},
   "source": [
    "# Embedding Analysis\n",
    "\n",
    "Research notebook for evaluating embedding quality and comparing LLM classifications against embedding nearest-neighbor baselines.\n",
    "\n",
    "**Requires:**\n",
    "- `data/embeddings/voyage_mcp_emb_2026-01-22.npy` — Voyage-4-large MCP embeddings\n",
    "- `data/embeddings/voyage_dwa_emb.npy` — Voyage-4-large DWA embeddings\n",
    "- `data/embeddings/mpnet_mcp_emb.npy` — all-mpnet-base-v2 MCP embeddings (optional)\n",
    "- `data/embeddings/mpnet_dwa_emb.npy` — all-mpnet-base-v2 DWA embeddings (optional)\n",
    "- `data/mcp/mcp_data_2026-01-22.csv` — cleaned MCP data\n",
    "- `data/onet/onet_data.csv` — O*NET tasks\n",
    "- `data/mcp/mcp_classification_teddy.csv` — manual ground-truth classifications\n",
    "- `data/mcp/gpt-4.1_v5.2_occ_gwa_iwa_dwa_task.csv` — GPT-4.1 v5.2 classifications\n",
    "\n",
    "The mpnet embeddings are not generated by the main pipeline. To regenerate them:\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embs = np.array(model.encode(texts, show_progress_bar=True), dtype=np.float32)\n",
    "np.save('data/embeddings/mpnet_mcp_emb.npy', embs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002setup",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport re\nfrom pathlib import Path\n\nDATA_DIR = Path(\"../data\")\n\n# ---------- Load MCP + O*NET data ----------\n# The full Jan 2026 dataset used for the original classification run\nmcp_df = pd.read_csv(DATA_DIR / \"mcp/raw/mcp_data_2026-01-22.csv\")\nonet_df = pd.read_csv(DATA_DIR / \"onet/onet_data.csv\")\n\nmcp_titles = mcp_df[\"title\"].tolist()\ndwa_titles_unique = onet_df[\"dwa_title\"].dropna().drop_duplicates().reset_index(drop=True).tolist()\n\nprint(f\"MCPs loaded:         {len(mcp_df):,}\")\nprint(f\"Unique DWA titles:   {len(dwa_titles_unique):,}\")\n\n# ---------- Load Voyage embeddings ----------\n# voyage_mcp_emb.npy = original full-dataset embeddings (8,957 MCPs x 1024-dim)\nvoyage_mcp_emb = np.load(DATA_DIR / \"embeddings/voyage_mcp_emb.npy\")\nvoyage_dwa_emb = np.load(DATA_DIR / \"embeddings/voyage_dwa_emb.npy\")\nprint(f\"Voyage MCP emb:  {voyage_mcp_emb.shape}\")\nprint(f\"Voyage DWA emb:  {voyage_dwa_emb.shape}\")\n\n# ---------- Load mpnet embeddings (optional) ----------\nmpnet_mcp_path = DATA_DIR / \"embeddings/mpnet_mcp_emb.npy\"\nmpnet_dwa_path = DATA_DIR / \"embeddings/mpnet_dwa_emb.npy\"\nif mpnet_mcp_path.exists() and mpnet_dwa_path.exists():\n    mpnet_mcp_emb = np.load(mpnet_mcp_path)\n    mpnet_dwa_emb = np.load(mpnet_dwa_path)\n    print(f\"mpnet MCP emb:   {mpnet_mcp_emb.shape}\")\n    print(f\"mpnet DWA emb:   {mpnet_dwa_emb.shape}\")\n    MPNET_AVAILABLE = True\nelse:\n    print(\"mpnet embeddings not found — skipping mpnet sections.\")\n    MPNET_AVAILABLE = False\n\n# ---------- L2 normalize ----------\ndef l2_normalize(X):\n    norms = np.linalg.norm(X, axis=1, keepdims=True)\n    norms[norms == 0] = 1\n    return X / norms\n\nvoyage_mcp_norm = l2_normalize(voyage_mcp_emb)\nvoyage_dwa_norm = l2_normalize(voyage_dwa_emb)\nif MPNET_AVAILABLE:\n    mpnet_mcp_norm = l2_normalize(mpnet_mcp_emb)\n    mpnet_dwa_norm = l2_normalize(mpnet_dwa_emb)\n\nprint(\"\\nSetup complete.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "a003md01",
   "metadata": {},
   "source": [
    "## Cosine Similarity Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cosine Similarity Distribution Analysis\n",
    "# ============================================================\n",
    "\n",
    "def print_stats(label, vals):\n",
    "    print(f\"    Mean:        {np.mean(vals):.4f}\")\n",
    "    print(f\"    Median:      {np.median(vals):.4f}\")\n",
    "    print(f\"    Std Dev:     {np.std(vals):.4f}\")\n",
    "    print(f\"    Min:         {np.min(vals):.4f}\")\n",
    "    print(f\"    Max:         {np.max(vals):.4f}\")\n",
    "    print(f\"    5th pctile:  {np.percentile(vals, 5):.4f}\")\n",
    "    print(f\"    25th pctile: {np.percentile(vals, 25):.4f}\")\n",
    "    print(f\"    75th pctile: {np.percentile(vals, 75):.4f}\")\n",
    "    print(f\"    95th pctile: {np.percentile(vals, 95):.4f}\")\n",
    "\n",
    "\n",
    "def run_distribution_analysis(mcp_emb, dwa_emb, model_name):\n",
    "    mcp_norm = l2_normalize(mcp_emb)\n",
    "    dwa_norm = l2_normalize(dwa_emb)\n",
    "\n",
    "    n_mcp = mcp_norm.shape[0]\n",
    "    n_dwa = dwa_norm.shape[0]\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"COSINE SIMILARITY DISTRIBUTION ANALYSIS  --  {model_name}\")\n",
    "    print(f\"  {n_mcp:,} MCP servers  |  {n_dwa:,} unique DWA titles\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    TOP_K = 10\n",
    "    chunk_size = 500\n",
    "    top1_sims = []\n",
    "    topk_means = []\n",
    "    all_means = []\n",
    "    rank_sims = [[] for _ in range(TOP_K)]\n",
    "\n",
    "    for i in range(0, n_mcp, chunk_size):\n",
    "        chunk = mcp_norm[i : i + chunk_size]\n",
    "        sims = chunk @ dwa_norm.T\n",
    "\n",
    "        for row in sims:\n",
    "            sorted_row = np.sort(row)[::-1]\n",
    "            top1_sims.append(sorted_row[0])\n",
    "            topk_means.append(sorted_row[:TOP_K].mean())\n",
    "            all_means.append(row.mean())\n",
    "            for k in range(TOP_K):\n",
    "                rank_sims[k].append(sorted_row[k])\n",
    "\n",
    "    top1_sims = np.array(top1_sims)\n",
    "    topk_means = np.array(topk_means)\n",
    "    all_means = np.array(all_means)\n",
    "\n",
    "    print()\n",
    "    print(\"--- A. MCP Embedding -> DWA Embeddings ---\")\n",
    "    print()\n",
    "    print(\"  Top-1 (closest single DWA)\")\n",
    "    print_stats(\"top1\", top1_sims)\n",
    "\n",
    "    print()\n",
    "    print(f\"  Top-{TOP_K} mean (avg of {TOP_K} closest)\")\n",
    "    print_stats(f\"top{TOP_K}\", topk_means)\n",
    "\n",
    "    print()\n",
    "    print(\"  All-DWA mean (baseline avg across all DWAs)\")\n",
    "    print_stats(\"all\", all_means)\n",
    "\n",
    "    print()\n",
    "    print(f\"  Similarity drop-off by rank position (1=closest, {TOP_K}={TOP_K}th closest):\")\n",
    "    print(f\"    {'Rank':>8}   {'Mean':>6}   {'Median':>6}      {'Std':>5}\")\n",
    "    for k in range(TOP_K):\n",
    "        arr = np.array(rank_sims[k])\n",
    "        print(f\"    {k+1:>8}   {arr.mean():.4f}   {np.median(arr):.4f}   {arr.std():.4f}\")\n",
    "\n",
    "    print()\n",
    "    return mcp_norm, dwa_norm\n",
    "\n",
    "\n",
    "run_distribution_analysis(voyage_mcp_emb, voyage_dwa_emb, \"Voyage-4-large\")\n",
    "\n",
    "if MPNET_AVAILABLE:\n",
    "    run_distribution_analysis(mpnet_mcp_emb, mpnet_dwa_emb, \"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005md02",
   "metadata": {},
   "source": [
    "## Example Similarity Pairs (Eyeball Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006pairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Example pairs at different similarity thresholds\n",
    "# For each model: show 2 examples each for DWA<->DWA,\n",
    "# MCP<->MCP, and MCP<->DWA at HIGH and MEDIUM similarity\n",
    "# ============================================================\n",
    "\n",
    "def show_example_pairs(mcp_norm, dwa_norm, mcp_titles_list, dwa_titles_list, model_name):\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"EXAMPLE SIMILARITY PAIRS  --  {model_name}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # --- DWA <-> DWA ---\n",
    "    print(\"\\n--- DWA <-> DWA ---\")\n",
    "    dwa_sims = dwa_norm @ dwa_norm.T\n",
    "    np.fill_diagonal(dwa_sims, 0)\n",
    "\n",
    "    flat_idx = np.argmax(dwa_sims)\n",
    "    i, j = divmod(flat_idx, dwa_sims.shape[1])\n",
    "    print(f\"\\n  HIGH similarity ({dwa_sims[i, j]:.4f}):\")\n",
    "    print(f\"    A: {dwa_titles_list[i]}\")\n",
    "    print(f\"    B: {dwa_titles_list[j]}\")\n",
    "\n",
    "    upper = dwa_sims[np.triu_indices_from(dwa_sims, k=1)]\n",
    "    median_sim = np.median(upper)\n",
    "    diff = np.abs(dwa_sims - median_sim)\n",
    "    np.fill_diagonal(diff, 999)\n",
    "    flat_idx = np.argmin(diff)\n",
    "    i, j = divmod(flat_idx, dwa_sims.shape[1])\n",
    "    print(f\"\\n  MEDIUM similarity ({dwa_sims[i, j]:.4f}, median={median_sim:.4f}):\")\n",
    "    print(f\"    A: {dwa_titles_list[i]}\")\n",
    "    print(f\"    B: {dwa_titles_list[j]}\")\n",
    "\n",
    "    # --- MCP <-> MCP ---\n",
    "    print(\"\\n--- MCP <-> MCP ---\")\n",
    "    sample_n = min(500, len(mcp_titles_list))\n",
    "    rng = np.random.default_rng(42)\n",
    "    sample_idx = rng.choice(len(mcp_titles_list), sample_n, replace=False)\n",
    "    mcp_sample = mcp_norm[sample_idx]\n",
    "    mcp_sample_titles = [mcp_titles_list[i] for i in sample_idx]\n",
    "\n",
    "    mcp_sims = mcp_sample @ mcp_sample.T\n",
    "    np.fill_diagonal(mcp_sims, 0)\n",
    "\n",
    "    flat_idx = np.argmax(mcp_sims)\n",
    "    i, j = divmod(flat_idx, mcp_sims.shape[1])\n",
    "    print(f\"\\n  HIGH similarity ({mcp_sims[i, j]:.4f}):\")\n",
    "    print(f\"    A: {mcp_sample_titles[i]}\")\n",
    "    print(f\"    B: {mcp_sample_titles[j]}\")\n",
    "\n",
    "    upper = mcp_sims[np.triu_indices_from(mcp_sims, k=1)]\n",
    "    median_sim = np.median(upper)\n",
    "    diff = np.abs(mcp_sims - median_sim)\n",
    "    np.fill_diagonal(diff, 999)\n",
    "    flat_idx = np.argmin(diff)\n",
    "    i, j = divmod(flat_idx, mcp_sims.shape[1])\n",
    "    print(f\"\\n  MEDIUM similarity ({mcp_sims[i, j]:.4f}, median={median_sim:.4f}):\")\n",
    "    print(f\"    A: {mcp_sample_titles[i]}\")\n",
    "    print(f\"    B: {mcp_sample_titles[j]}\")\n",
    "\n",
    "    # --- MCP <-> DWA ---\n",
    "    print(\"\\n--- MCP <-> DWA ---\")\n",
    "    cross_sims = mcp_sample @ dwa_norm.T\n",
    "\n",
    "    flat_idx = np.argmax(cross_sims)\n",
    "    i, j = divmod(flat_idx, cross_sims.shape[1])\n",
    "    print(f\"\\n  HIGH similarity ({cross_sims[i, j]:.4f}):\")\n",
    "    print(f\"    MCP: {mcp_sample_titles[i]}\")\n",
    "    print(f\"    DWA: {dwa_titles_list[j]}\")\n",
    "\n",
    "    upper = cross_sims.flatten()\n",
    "    median_sim = np.median(upper)\n",
    "    diff = np.abs(cross_sims - median_sim)\n",
    "    flat_idx = np.argmin(diff)\n",
    "    i, j = divmod(flat_idx, cross_sims.shape[1])\n",
    "    print(f\"\\n  MEDIUM similarity ({cross_sims[i, j]:.4f}, median={median_sim:.4f}):\")\n",
    "    print(f\"    MCP: {mcp_sample_titles[i]}\")\n",
    "    print(f\"    DWA: {dwa_titles_list[j]}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "show_example_pairs(voyage_mcp_norm, voyage_dwa_norm, mcp_titles, dwa_titles_unique, \"Voyage-4-large\")\n",
    "\n",
    "if MPNET_AVAILABLE:\n",
    "    show_example_pairs(mpnet_mcp_norm, mpnet_dwa_norm, mcp_titles, dwa_titles_unique, \"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007md03",
   "metadata": {},
   "source": [
    "## Classification Comparison: Manual + GPT-4.1 v5.2 vs Embedding Nearest Neighbors\n",
    "\n",
    "For each (MCP, selected DWA) pair in the ground-truth classifications, this section computes the cosine similarity rank of that DWA relative to all 2,083 unique DWAs. This validates how well the embedding retrieval step captures the correct DWAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008comp",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Compare Teddy & GPT-4.1 DWA selections against embedding\n# nearest neighbors.\n# ============================================================\n\nteddy_class = pd.read_csv(DATA_DIR / \"mcp/mcp_classification_teddy.csv\")\ngpt_class = pd.read_csv(DATA_DIR / \"mcp/gpt-4.1_v5.2_occ_gwa_iwa_dwa_task.csv\")\n\nprint(f\"Teddy classifications loaded: {len(teddy_class)} MCPs\")\nprint(f\"GPT-4.1 classifications loaded: {len(gpt_class)} MCPs\")\n\n# --- Build lookup tables ---\ndef normalize_dwa_text(text):\n    text = text.strip().lower()\n    text = text.rstrip(\".\")\n    text = re.sub(r\"[,;]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text\n\ndwa_lookup = {normalize_dwa_text(d): i for i, d in enumerate(dwa_titles_unique)}\nmcp_title_to_idx = {str(t).strip(): i for i, t in enumerate(mcp_titles)}\nmcp_url_to_idx = {str(u).strip(): i for i, u in enumerate(mcp_df[\"url\"].values)}\n\nprint(f\"DWA lookup:  {len(dwa_lookup):,} entries\")\nprint(f\"MCP lookup:  {len(mcp_title_to_idx):,} entries\")\n\n\ndef compute_dwa_ranks(class_df, source_name, mcp_norm, dwa_norm, dwa_col=\"dwa\"):\n    results = []\n    n_matched_mcps = 0\n    n_unmatched_mcps = 0\n    n_matched_dwas = 0\n    n_unmatched_dwas = 0\n    unmatched_examples = []\n\n    valid = class_df[\n        class_df[dwa_col].notna()\n        & (class_df[dwa_col].astype(str).str.strip() != \"\")\n        & (class_df[\"occ_relevant\"].astype(str).str.strip().str.lower() == \"yes\")\n    ].copy()\n\n    for _, row in valid.iterrows():\n        title = str(row[\"title\"]).strip()\n        url = str(row.get(\"url\", \"\")).strip()\n\n        mcp_idx = mcp_title_to_idx.get(title)\n        if mcp_idx is None:\n            mcp_idx = mcp_url_to_idx.get(url)\n        if mcp_idx is None:\n            n_unmatched_mcps += 1\n            continue\n\n        mcp_vec = mcp_norm[mcp_idx : mcp_idx + 1]\n        n_matched_mcps += 1\n\n        all_sims = (mcp_vec @ dwa_norm.T).flatten()\n        sorted_indices = np.argsort(-all_sims)\n\n        rank_lookup = np.empty(len(all_sims), dtype=int)\n        rank_lookup[sorted_indices] = np.arange(1, len(all_sims) + 1)\n\n        top1_sim = float(all_sims[sorted_indices[0]])\n        top1_dwa_text = dwa_titles_unique[sorted_indices[0]]\n\n        dwa_strings = [t.strip() for t in str(row[dwa_col]).split(\";\") if t.strip()]\n\n        for dwa_str in dwa_strings:\n            norm_key = normalize_dwa_text(dwa_str)\n            if norm_key in dwa_lookup:\n                dwa_idx = dwa_lookup[norm_key]\n                sim = float(all_sims[dwa_idx])\n                rank = int(rank_lookup[dwa_idx])\n                n_matched_dwas += 1\n                results.append({\n                    \"source\": source_name,\n                    \"mcp_title\": title,\n                    \"selected_dwa\": dwa_str,\n                    \"cosine_similarity\": round(sim, 4),\n                    \"rank\": rank,\n                    \"total_dwas\": len(dwa_titles_unique),\n                    \"percentile\": round((1 - rank / len(dwa_titles_unique)) * 100, 2),\n                    \"top1_similarity\": round(top1_sim, 4),\n                    \"top1_dwa\": top1_dwa_text,\n                    \"sim_gap_from_top1\": round(top1_sim - sim, 4),\n                })\n            else:\n                n_unmatched_dwas += 1\n                unmatched_examples.append((title, dwa_str))\n\n    print(f\"\\n  {source_name}\")\n    print(f\"  MCPs matched: {n_matched_mcps}  |  not found: {n_unmatched_mcps}\")\n    print(f\"  DWAs matched: {n_matched_dwas}  |  not matched: {n_unmatched_dwas}\")\n    if unmatched_examples:\n        print(\"  Sample unmatched DWAs:\")\n        for title, dwa in unmatched_examples[:5]:\n            print(f\"    [{title}] {dwa[:80]}\")\n\n    return pd.DataFrame(results)\n\n\ndef print_rank_summary(label, df):\n    if df.empty:\n        print(f\"{label}: No matched DWAs to analyze.\")\n        return\n    print()\n    print(\"=\" * 60)\n    print(f\"  {label} -- Summary Statistics\")\n    print(\"=\" * 60)\n    print(f\"  Total (MCP, DWA) pairs: {len(df):,}\")\n    print(f\"  Unique MCPs:            {df['mcp_title'].nunique()}\")\n    print()\n    print(\"  Cosine Similarity of Selected DWAs:\")\n    print(f\"    Mean:    {df['cosine_similarity'].mean():.4f}\")\n    print(f\"    Median:  {df['cosine_similarity'].median():.4f}\")\n    print()\n    print(f\"  Rank among {df['total_dwas'].iloc[0]:,} unique DWAs:\")\n    print(f\"    Mean rank:   {df['rank'].mean():.0f}\")\n    print(f\"    Median rank: {df['rank'].median():.0f}\")\n    print(f\"    % in top 10:   {(df['rank'] <= 10).mean()*100:.1f}%\")\n    print(f\"    % in top 50:   {(df['rank'] <= 50).mean()*100:.1f}%\")\n    print(f\"    % in top 80:   {(df['rank'] <= 80).mean()*100:.1f}%\")\n    print(f\"    % in top 100:  {(df['rank'] <= 100).mean()*100:.1f}%\")\n\n\nmodels_to_run = [('Voyage-4-large', voyage_mcp_norm, voyage_dwa_norm)]\nif MPNET_AVAILABLE:\n    models_to_run.append(('all-mpnet-base-v2', mpnet_mcp_norm, mpnet_dwa_norm))\n\nfor model_label, mcp_n, dwa_n in models_to_run:\n    print('\\n' + '#' * 70)\n    print(f'#  {model_label}')\n    print('#' * 70)\n\n    teddy_ranks = compute_dwa_ranks(teddy_class, f'Teddy (manual) [{model_label}]', mcp_n, dwa_n)\n    gpt_ranks = compute_dwa_ranks(gpt_class, f'GPT-4.1 (v5.2) [{model_label}]', mcp_n, dwa_n)\n\n    print_rank_summary(f'Teddy (manual) [{model_label}]', teddy_ranks)\n    print_rank_summary(f'GPT-4.1 (v5.2) [{model_label}]', gpt_ranks)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}